{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec4c974d-b548-40c5-8b82-ed04e6e75be6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install openai==0.28.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d552ecd-d35d-4c83-925a-cd76b5fc9717",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8b41b97-038b-400a-b3be-16b383902630",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import language_tool_python\n",
    "import pandas as pd\n",
    "import openai\n",
    "import yaml\n",
    "from bs4 import BeautifulSoup\n",
    "import html\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2c9436f-538a-451d-98a7-dbcb3c83dd21",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59f4d6fc-2890-4d63-8012-8c270c934b68",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_path = r\"your_path.parquet\"\n",
    "data = pd.read_parquet(data_path)\n",
    "tool = language_tool_python.LanguageTool('en-US')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b3f6821-06cc-451b-8152-ce0f511ac6ad",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Open AI passwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d22404e5-fe6c-48fc-8d6a-65d2138f83dd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_passwords_from_yaml(file_path_):\n",
    "    with open(file_path_, 'r') as yaml_file:\n",
    "        password = yaml.safe_load(yaml_file)\n",
    "    return password\n",
    "\n",
    "file_path = ''\n",
    "passwords = read_passwords_from_yaml(file_path)\n",
    "openai.api_type = passwords['openai.api_type']\n",
    "openai.api_version = passwords['openai.api_version']\n",
    "openai.api_base = passwords['openai.api_base']\n",
    "openai.api_key = passwords['openai.api_key']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5157fef-716e-4160-8913-f63b34b2b1ae",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Removing html tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5353526e-2286-4544-9000-149582b060e5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def remove_html_tags(input_string):\n",
    "    soup = BeautifulSoup(input_string, \"html.parser\")  # to remove <p> etc\n",
    "    text_without_tags = soup.get_text()\n",
    "    text_without_nbsp = text_without_tags.replace('\\xa0', ' ')  # replacing non-breaking space with regular space\n",
    "    return html.unescape(text_without_nbsp)  # to decode &nbsp to space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef9e2a52-111f-463c-84c7-e418ca640da7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### GPT's Turn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c973cf08-a582-4b5a-8125-462b7a38a996",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with open(\"/template.html\", \"r\") as file:\n",
    "    html_template = file.read()\n",
    "\n",
    "\n",
    "def get_gpt_correction(original_text):\n",
    "    prompt = (\n",
    "    \"Please proofread thoroughly the given text, identifying and correcting any grammatical, punctuation or spelling,\"\n",
    "    \"errors, as well as improving sentence structure and clarity where necessary. Ensure that the document is polished and adheres\"\n",
    "    \"to the highest standards of written English. DO NOT suggest word improvements. Try to classify the errors into the three main categories, Grammar\"\n",
    "    \"Spelling and Punctuation\"\n",
    "    \"Return ONLY a JSON object using double quotes based on the template provided. \"\n",
    "    \"Below is the template with descriptions for each field:\"\n",
    "    \"\\n\"\n",
    "    \"{\\n\"\n",
    "    '   \"Number of mistakes\": \"Total number of mistakes found in the text\",\\n'\n",
    "    '   \"mistakes\": [\\n'\n",
    "    '       {\\n'\n",
    "    '           \"original_text\": \"Text that will be replaced due to the mistake\",\\n'\n",
    "    '           \"new_text\": \"Corrected version of the mistake\",\\n'\n",
    "    '           \"explanation\": \"Reason why this is considered a mistake\",\\n'\n",
    "    '           \"mistake_type\": \"Type of the error (e.g. Grammar, Punctuation, Spelling).\\n'\n",
    "    '           \"nth occurrence\": \"n, where \"n\" represents which word is wrong out of all the common words.For example in a sentence\\n'\n",
    "    '                              \"Thank you for you time,Thank you.\" , the word \"you\" appears three times, and the word which is wrong is the second \"you\"\\n'\n",
    "    '                              that needs to be replaced with \"your\". So n=2'\n",
    "    '       }\\n'\n",
    "    '   ]\\n'\n",
    "    \"}\\n\"\n",
    "    \n",
    ")\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        engine=\"gpt-4-32k\",  # The deployment name you chose when you deployed the ChatGPT or GPT-4 model.\n",
    "        # \"text-davinci-003\" \"gpt-35-turbo\" \"gpt-4-32k\"\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": prompt},\n",
    "            {\"role\": \"user\", \"content\": original_text}\n",
    "            \n",
    "        ]\n",
    "\n",
    "    )\n",
    "\n",
    "    response_content = response['choices'][0]['message']['content']\n",
    "    # Count occurrences of the key 'original_text' in the response content\n",
    "    gpt_error_count = response_content.count(\"original_text\")\n",
    "\n",
    "    return response_content, gpt_error_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab09ff41-3711-446d-b606-19e9f75d4158",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def split_text(text, max_length=80):\n",
    "    lines = []\n",
    "    current_line = []\n",
    "    words = text.split()\n",
    "    for word in words:\n",
    "        if len(' '.join(current_line + [word])) <= max_length:\n",
    "            current_line.append(word)\n",
    "        else:\n",
    "            lines.append(' '.join(current_line))\n",
    "            current_line = [word]\n",
    "    # Add the last line\n",
    "    if current_line:\n",
    "        lines.append(' '.join(current_line))\n",
    "    for line in lines:\n",
    "        print(line)\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73aa3065-02fe-4de7-8397-650221923d53",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Generating HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e249db05-e370-43c0-9f52-3b0e6b2de4ca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def generate_html_from_response(response, original_text, essay_number):\n",
    "\n",
    "    response_data = json.loads(response)\n",
    "    #copying original_text\n",
    "    corrected_version = original_text[:]\n",
    "\n",
    "    candidate_id = response_data.get('candidate_id')\n",
    "    q_id = response_data.get('Q_ID')\n",
    "    mistakes = response_data[\"mistakes\"]\n",
    "    processed_positions = {}\n",
    "\n",
    "    start_pos = 0\n",
    "    occurrence_counter = {}\n",
    "    mistake_groups = {}  # Dictionary to hold mistakes grouped by type\n",
    "\n",
    "    for mistake in mistakes:\n",
    "\n",
    "        original_mistake = mistake[\"original_text\"]\n",
    "        mistake_type = mistake[\"mistake_type\"] #MA = Mistake Analysis\n",
    "        explanation = mistake[\"explanation\"] #MA\n",
    "        new_text = mistake[\"new_text\"]\n",
    "\n",
    "        nth_occurrence = mistake.get(\"nth occurrence\",1)\n",
    "        occurrence_counter.setdefault(original_mistake,1)\n",
    "\n",
    "\n",
    "        mistake_item = f'<li class=\"analysis-item\">{original_mistake} &rarr; {new_text} - {explanation}</li>' #MA\n",
    "        # Group mistakes by type - MA\n",
    "        if mistake_type not in mistake_groups:\n",
    "            mistake_groups[mistake_type] = []\n",
    "        mistake_groups[mistake_type].append(mistake_item)\n",
    "\n",
    "\n",
    "\n",
    "        # finds the starting index of the first occurance of original_mistake\n",
    "        # Check if the mistake ends with a punctuation mark\n",
    "        if any(original_mistake.endswith(punct) for punct in [',',';']):\n",
    "            # If so, include the punctuation in the regex pattern\n",
    "            regex_pattern = r'\\b{}\\b(?=[.,!?;:])'.format(re.escape(original_mistake.rstrip('.,!?;:')))\n",
    "        else:\n",
    "            # Otherwise, use a standard word boundary\n",
    "            regex_pattern = r'\\b{}\\b'.format(re.escape(original_mistake))\n",
    "\n",
    "        # Find the position of the mistake\n",
    "        mistake_pos = next((m.start() for m in re.finditer(regex_pattern, corrected_version)), -1) # in order to not replace a part of a word,ex. in \"Yours\" not replace \"You\" with smthing\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if original_mistake in processed_positions and mistake_pos in processed_positions[original_mistake]: # In order to not replace a mistake in original text more times that it should.\n",
    "            continue\n",
    "\n",
    "\n",
    "        while True:\n",
    "            if mistake_pos != -1: # If .find doesn't find the substring, it returns -1\n",
    "                if occurrence_counter[original_mistake] == int(nth_occurrence):\n",
    "                    new_text = f'<span class=\"{mistake[\"mistake_type\"]}\">{mistake[\"new_text\"]}</span>' #the replacement\n",
    "                    corrected_version = corrected_version[:mistake_pos] + new_text + corrected_version[mistake_pos + len(original_mistake):]\n",
    "                    start_pos = mistake_pos + len(new_text)\n",
    "                \n",
    "\n",
    "                    # Update the processed_positions dictionary here:\n",
    "                    if original_mistake not in processed_positions:\n",
    "                        processed_positions[original_mistake] = []\n",
    "                    processed_positions[original_mistake].append(mistake_pos)\n",
    "\n",
    "                    break\n",
    "                else:\n",
    "                    # If the occurrence is not the one we want to replace, update the occurrence counter\n",
    "                    occurrence_counter[original_mistake] += 1\n",
    "\n",
    "                    # Move the start_pos beyond the current occurrence to search for the next one\n",
    "                    start_pos = mistake_pos + len(original_mistake)\n",
    "                    mistake_pos = start_pos + next((m.start() for m in re.finditer(regex_pattern, corrected_version[start_pos:])),-1)\n",
    "\n",
    "\n",
    "\n",
    "    # Compile the mistake analysis html\n",
    "    mistake_analysis_list = []\n",
    "    for mistake_type, items in mistake_groups.items():\n",
    "        # Add mistake type header (e.g., \"Grammar:\")\n",
    "        mistake_analysis_list.append(\n",
    "            f'<li class=\"analysis-item\"><strong><span class=\"{mistake_type}\">{mistake_type.capitalize()}</span>:</strong></li>')\n",
    "        # Add the mistakes for this type\n",
    "        mistake_analysis_list.extend(items)\n",
    "    \n",
    "    mistake_analysis_html = \"\\n\".join(mistake_analysis_list) #MA\n",
    "\n",
    "    with open(\"/template.html\", \"r\") as file:\n",
    "        template = file.read()\n",
    "\n",
    "    html_output = template.replace(\"{{candidate_id}}\", candidate_id).replace(\"{{Q_ID}}\",q_id).replace(\"{{original_text}}\", original_text).replace(\"{{corrected_text}}\", corrected_version).replace(\"{{mistake_analysis}}\", mistake_analysis_html)\n",
    "\n",
    "    return html_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eaec6ad1-78b8-4483-9976-7a7fca3620d3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Adding Candidate ID to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "804ef8fb-79a4-4bab-a4dd-8e6b47fca817",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def add_candidate_question_id(gpt_response, essay_number):\n",
    "    try:\n",
    "        gpt_response = json.loads(gpt_response)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Warning: Unable to parse response for essay #{essay_number} as JSON. Skipping...\")\n",
    "        print(f\"Error while parsing JSON: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "    additional_info = {\n",
    "        'candidate_id': data['Candidate_ID'].iloc[essay_number],\n",
    "        'Q_ID': data['Q_ID'].iloc[essay_number]  # Assuming 'Q_ID' is a column in your DataFrame\n",
    "    }\n",
    "    gpt_response = {**additional_info, **gpt_response}\n",
    "    gpt_response = json.dumps(gpt_response)\n",
    "    return gpt_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e904ed47-d725-4127-9d18-f2d0b4e5b796",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "all_html_content = \"\"\n",
    "# all_response_data = []\n",
    "\n",
    "for essay_number, doc in enumerate(data['Answer_Selected'].iloc[0:1]):\n",
    "    clean_text = remove_html_tags(doc)\n",
    "    gpt_response, gpt_error_count = get_gpt_correction(clean_text)\n",
    "    gpt_response = add_candidate_question_id(gpt_response, essay_number)\n",
    "    print(gpt_response)\n",
    "    \n",
    "    html_content = generate_html_from_response(gpt_response, clean_text, essay_number)\n",
    "    all_html_content += html_content\n",
    "\n",
    "    # all_response_data.append(response_data)\n",
    " \n",
    "with open(\"/outputgpt.html\", \"w\") as file:\n",
    "    file.write(all_html_content)\n",
    "\n",
    "# with open(\"/output.json\", 'w', encoding=\"utf-8\") as outfile:\n",
    "#     json.dump(all_response_data, outfile, indent=4)\n",
    "\n",
    "tool.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20b4837f-e5e1-415d-b1fc-5d96d3cf48cc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Querrying JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a0a9873-b76e-40d9-827f-e7df76662623",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "\n",
    "# mistake_type_counter = Counter()\n",
    "\n",
    "# for entry in all_response_data:\n",
    "#     for mistake in entry.get('mistakes', []):\n",
    "#         mistake_type = mistake.get('mistake_type')\n",
    "#         if mistake_type:\n",
    "#             mistake_type_counter[mistake_type] += 1\n",
    "\n",
    "# mistake_type_counter"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "GPT ONLY_v6",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
